{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a href=\"https://colab.research.google.com/github/ypeleg/keras_rl_tutorial/blob/master/0.0.2%20-%20Introduction%20-%20Keras%20Functional%20API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <center><strong><h5>Reinforcement Learning Tutorial!</h5></strong></center>\n",
    "    <center><strong><h2>0.0.2 Keras Functional API</h2></strong></center> \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Functional API:\n",
    "##  `tl;dr:  next_layer(**params)(previous_layer)`\n",
    "\n",
    "Example (LSTM):\n",
    "```python\n",
    "x = LSTM(64)(x)\n",
    "x = Dense(2, activation='relu')(x)\n",
    "```\n",
    "\n",
    "Example (CNN):\n",
    "```python\n",
    "x = Conv2D(64, 3, 3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(2, activation='relu')(x)\n",
    "```\n",
    "\n",
    "Example (Concatenate): \n",
    "```python\n",
    "lay1 = Dense(50, activation = 'relu')(x)\n",
    "lay2 = Dense(50, activation = 'relu')(x)\n",
    "x = concatenate([lay1, lay2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style =\"width:50%;\" src=\"images/intro.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised Learning models.\n",
    "- Represented by a weights tensor.\n",
    "- Trained by Gradient Descent.\n",
    "<img src=\"imgs/single_layer.png\" width=\"65%\" />\n",
    "(Source: Python Machine Learning, S. Raschka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We use a **gradient descent** optimization algorithm to learn the _Weights Coefficients_ of the model.\n",
    "<br><br>\n",
    "- In every **epoch** (pass over the training set), we update the weight vector $w$ using the following update rule:\n",
    "\n",
    "$$\n",
    "w = w + \\Delta w, \\text{where } \\Delta w = - \\eta \\nabla J(w)\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In other words, we computed the gradient based on the whole training set and updated the weights of the model by taking a step into the **opposite direction** of the gradient $ \\nabla J(w)$. \n",
    "\n",
    "In order to fin the **optimal weights of the model**, we optimized an objective function (e.g. the Sum of Squared Errors (SSE)) cost function $J(w)$. \n",
    "\n",
    "Furthermore, we multiply the gradient by a factor, the learning rate $\\eta$ , which we choose carefully to balance the **speed of learning** against the risk of overshooting the global minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **gradient descent optimization**, we update all the **weights simultaneously** after each epoch, and we define the _partial derivative_ for each weight $w_j$ in the weight vector $w$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} J(w) = \\sum_{i} ( y^{(i)} - a^{(i)} )  x^{(i)}_j\n",
    "$$\n",
    "\n",
    "**Note**: _The superscript $(i)$ refers to the i-th sample. The subscript $j$ refers to the j-th dimension/feature_\n",
    "\n",
    "\n",
    "Here $y^{(i)}$ is the target class label of a particular sample $x^{(i)}$ , and $a^{(i)}$ is the **activation** of the neuron \n",
    "\n",
    "(which is a linear function in the special case of _Perceptron_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **activation function** $\\phi(\\cdot)$ as follows:\n",
    "\n",
    "$$\n",
    "\\phi(z) = z = a = \\sum_{j} w_j x_j = \\mathbf{w}^T \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we used the **activation** $\\phi(z)$ to compute the gradient update, we may use a **threshold function** _(Heaviside function)_ to squash the continuous-valued output into binary class labels for prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\phi(z) \\geq 0 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/multi-layers-1.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how to connect **multiple single neurons** to a **multi-layer feedforward neural network**; this special type of network is also called a **multi-layer perceptron** (MLP). \n",
    "\n",
    "The figure shows the concept of an **MLP** consisting of three layers: one _input_ layer, one _hidden_ layer, and one _output_ layer. \n",
    "\n",
    "The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer, respectively. \n",
    "\n",
    "If such a network has **more than one hidden layer**, we also call it a **deep artificial neural network**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we denote the `ith` activation unit in the `lth` layer as $a_i^{(l)}$ , and the activation units $a_0^{(1)}$ and \n",
    "$a_0^{(2)}$ are the **bias units**, respectively, which we set equal to $1$. \n",
    "<br><br>\n",
    "The _activation_ of the units in the **input layer** is just its input plus the bias unit:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = [a_0^{(1)}, a_1^{(1)}, \\ldots, a_m^{(1)}]^T = [1, x_1^{(i)}, \\ldots, x_m^{(i)}]^T\n",
    "$$\n",
    "<br><br>\n",
    "**Note**: $x_j^{(i)}$ refers to the jth feature/dimension of the ith sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Notation (usually) Adopted\n",
    "\n",
    "The terminology around the indices (subscripts and superscripts) may look a little bit confusing at first. \n",
    "<br><br>\n",
    "\n",
    "You may wonder why we wrote $w_{j,k}^{(l)}$ and not $w_{k,j}^{(l)}$ to refer to \n",
    "the **weight coefficient** that connects the *kth* unit in layer $l$ to the jth unit in layer $l+1$. \n",
    "<br><br>\n",
    "\n",
    "What may seem a little bit quirky at first will make much more sense later when we **vectorize** the neural network representation. \n",
    "<br><br>\n",
    "\n",
    "For example, we will summarize the weights that connect the input and hidden layer by a matrix \n",
    "$$ W^{(1)} \\in \\mathbb{R}^{h√ó[m+1]}$$\n",
    "\n",
    "where $h$ is the number of hidden units and $m + 1$ is the number of hidden units plus bias unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/multi-layers-2.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output.\n",
    "\n",
    "* Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later.\n",
    "\n",
    "* We backpropagate the error, find its derivative with respect to each weight in the network, and update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/logistic_function.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/fwd_step.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/fwd_step_net.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5678486b-caf4-440b-be62-2f1286982c71"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights of each neuron are learned by **gradient descent**, where each neuron's error is derived with respect to it's weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/bkwd_step_net.png\" width=\"50%\" />\n",
    "\n",
    "_(Source: Python Machine Learning, S. Raschka)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimization is done for each layer with respect to the previous layer in a technique known as **BackPropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/backprop.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "\n",
    "\n",
    "- Creating models with keras's by the Sequential API is easy and simple, but it is impossible to create complex model structures, which we would need.\n",
    "- For instance, inception or residual net structure is impossible to implement using Sequential API since they require operations such as layer merging and multiple outputs\n",
    "\n",
    "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "This guide assumes that you are already familiar with the `Sequential` model.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"http://www.deeplearningmodel.net/img/googlenet/googlenet_block.png\" style=\"width: 300px\"/>\n",
    "<center> **Inception Module in GoogleNet** </center>\n",
    "\n",
    "<img src=\"http://cv-tricks.com/wp-content/uploads/2017/03/600x299xResNet.png.pagespeed.ic.M1J-VkbWPB.png\" style=\"width: 300px\"/>\n",
    "<center> **Resudiual Structure in ResNet** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input and multi-output models\n",
    "\n",
    "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n",
    "\n",
    "Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\n",
    "The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n",
    "\n",
    "Here's what our model looks like:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/keras.io/img/multi-input-multi-output-graph.png\" alt=\"multi-input-multi-output-graph\" style=\"width: 400px;\"/>\n",
    "\n",
    "Let's implement it with the functional API.\n",
    "\n",
    "The main input will receive the headline, as a sequence of integers (each integer encodes a word).\n",
    "The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.\n",
    "\n",
    "```python\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# Note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)\n",
    "```\n",
    "\n",
    "Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.\n",
    "\n",
    "```python\n",
    "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "```\n",
    "\n",
    "At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:\n",
    "\n",
    "```python\n",
    "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
    "x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "```\n",
    "\n",
    "This defines a model with two inputs and two outputs:\n",
    "\n",
    "```python\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "```\n",
    "\n",
    "We compile the model and assign a weight of 0.2 to the auxiliary loss.\n",
    "To specify different `loss_weights` or `loss` for each different output, you can use a list or a dictionary.\n",
    "Here we pass a single loss as the `loss` argument, so the same loss will be used on all outputs.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "```\n",
    "\n",
    "We can train the model by passing it lists of input arrays and target arrays:\n",
    "\n",
    "```python\n",
    "model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "```\n",
    "\n",
    "Since our inputs and outputs are named (we passed them a \"name\" argument),\n",
    "we could also have compiled the model via:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "              loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "\n",
    "# And trained it via:\n",
    "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "          {'main_output': labels, 'aux_output': labels},\n",
    "          epochs=50, batch_size=32)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of layer \"node\"\n",
    "\n",
    "Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...\n",
    "\n",
    "In previous versions of Keras, you could obtain the output tensor of a layer instance via `layer.get_output()`, or its output shape via `layer.output_shape`. You still can (except `get_output()` has been replaced by the property `output`). But what if a layer is connected to multiple inputs?\n",
    "\n",
    "As long as a layer is only connected to one input, there is no confusion, and `.output` will return the one output of the layer:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(280, 256))\n",
    "\n",
    "lstm = LSTM(32)\n",
    "encoded_a = lstm(a)\n",
    "```\n",
    "\n",
    "So then:\n",
    "```python\n",
    ">>>lstm.output == encoded_a\n",
    "True\n",
    "```\n",
    "Not so if the layer has multiple inputs:\n",
    "```python\n",
    "a = Input(shape=(280, 256))\n",
    "b = Input(shape=(280, 256))\n",
    "\n",
    "lstm = LSTM(32)\n",
    "encoded_a = lstm(a)\n",
    "encoded_b = lstm(b)\n",
    "\n",
    "lstm.output\n",
    "```\n",
    "```\n",
    ">> AttributeError: Layer lstm_1 has multiple inbound nodes,\n",
    "hence the notion of \"layer output\" is ill-defined.\n",
    "Use `get_output_at(node_index)` instead.\n",
    "```\n",
    "\n",
    "Okay then. The following works:\n",
    "\n",
    "```python\n",
    ">>> lstm.get_output_at(0) == encoded_a\n",
    "True\n",
    "\n",
    ">>> lstm.get_output_at(1) == encoded_b\n",
    "True\n",
    "```\n",
    "\n",
    "Simple enough, right?\n",
    "\n",
    "The same is true for the properties `input_shape` and `output_shape`: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by `layer.output_shape`/`layer.input_shape`. But if, for instance, you apply the same `Conv2D` layer to an input of shape `(32, 32, 3)`, and then to an input of shape `(64, 64, 3)`, the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32, 32, 3))\n",
    "b = Input(shape=(64, 64, 3))\n",
    "\n",
    "conv = Conv2D(16, (3, 3), padding='same')\n",
    "conved_a = conv(a)\n",
    "\n",
    "# Only one input so far, the following will work:\n",
    "conv.input_shape == (None, 32, 32, 3)\n",
    ">>>True\n",
    "\n",
    "conved_b = conv(b)\n",
    "# now the `.input_shape` property wouldn't work, but this does:\n",
    ">>> conv.get_input_shape_at(0) == (None, 32, 32, 3)\n",
    "True\n",
    "\n",
    ">>> conv.get_input_shape_at(1) == (None, 64, 64, 3)\n",
    "True\n",
    "\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to work!\n",
    "### As always: MNIST\n",
    "- Also known as digits dataset in scikit-learn\n",
    "- url: http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('keras_rl_tutorial'): os.system('git clone https://github.com/ypeleg/keras_rl_tutorial/')\n",
    "os.chdir('keras_rl_tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tachles import load_mnist, show_mnist_teaser, PlotLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5d8bfe410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "show_mnist_teaser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn! \n",
    "- Create a simple model just like the demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (60000, 28, 28, 1)  y:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print 'x shape: ', x_train.shape, ' y: ', y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implement a simple Convolution network here! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow start: Single input & output\n",
    "- Model with only single input & output\n",
    "- Such structure is able to create using Sequential API as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image\n",
    "from keras.layers import Input, Activation, Dense, Flatten, MaxPooling2D, Conv2D, LSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (60000, 28, 28, 1)  y:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print 'x shape: ', x_train.shape, ' y: ', y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5ac3cf550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5c6b0e950>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "27168/60000 [============>.................] - ETA: 38s - loss: 0.2970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75e1e5c1e709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPlotLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yam/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/home/yam/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yam/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/home/yam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape = x_train.shape[1:])\n",
    "\n",
    "activation_1 = Activation('relu')(input_layer)\n",
    "hidden_layer = Dense(50)(activation_1)\n",
    "activation_2 = Activation('relu')(hidden_layer)\n",
    "flat = Flatten()(activation_2)\n",
    "output_layer = Dense(10, activation = 'softmax')(flat)\n",
    "\n",
    "model = Model(inputs = input_layer, outputs = output_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[PlotLearning()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging layers\n",
    "- Sometimes, it is necessary to merge layers (e.g., GoogleNet or ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. concatenate\n",
    "- concatenate() simply merges results of two or more layers\n",
    "- For instance, assume there are two layers to be concatenated, whose results are\n",
    "**[x1, x2, ..., xn]** and **[y1, y2, ..., yn]**. Then, concatenated layer would be **[x1, ..., xn, ..., y1, ..., yn]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape = X_data.shape[1:])\n",
    "activation_1 = Activation('relu')(input_layer)\n",
    "hidden_layer_1 = Dense(50, activation = 'relu')(activation_1)\n",
    "hidden_layer_2 = Dense(50, activation = 'relu')(activation_1)\n",
    "concat_layer = concatenate([hidden_layer_1, hidden_layer_2])\n",
    "print(hidden_layer_1.shape)\n",
    "print(hidden_layer_2.shape)\n",
    "print(concat_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. add, subtract, multiply, average, maximum\n",
    "- Such layers perform element-wise operations over all corresponding elements of two or more layers\n",
    "- Hence, dimensionality of the input layers are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating layers\n",
    "input_layer = Input(shape = X_data.shape[1:])\n",
    "activation_1 = Activation('relu')(input_layer)\n",
    "hidden_layer_1 = Dense(50, activation = 'relu')(activation_1)\n",
    "hidden_layer_2 = Dense(50, activation = 'relu')(activation_1)\n",
    "add_layer = add([hidden_layer_1, hidden_layer_2])\n",
    "print(hidden_layer_1.shape)\n",
    "print(hidden_layer_2.shape)\n",
    "print(add_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. dot\n",
    "- dot() performs inner product operation between two layer results\n",
    "- 'axes' should be defined to perform the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating layers\n",
    "input_layer = Input(shape = X_data.shape[1:])\n",
    "activation_1 = Activation('relu')(input_layer)\n",
    "hidden_layer_1 = Dense(50, activation = 'relu')(activation_1)\n",
    "hidden_layer_2 = Dense(50, activation = 'relu')(activation_1)\n",
    "dot_layer = dot([hidden_layer_1, hidden_layer_2], axes = -1)\n",
    "print(hidden_layer_1.shape)\n",
    "print(hidden_layer_2.shape)\n",
    "print(dot_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(2, ))\n",
    "hidden = Dense(2)(visible)\n",
    "model = Model(inputs=visible, outputs=hidden)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: \n",
    "\n",
    "#### Exe1:\n",
    "- Build the following network: Input -> Dense(10) -> Dense(2) -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(2, ))\n",
    "\n",
    "# x = ...\n",
    "\n",
    "out = Dense(2)(x)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exe2: \n",
    "- Build the following network: Input -> Dense(2) -> Dense(5) -> Dense(7) -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your model goes here..\n",
    "# ...\n",
    "# .. \n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exe3: \n",
    "- Build the following network: Input -> Dense(3) -> Dense(5) -> Dense(3) -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your model goes here..\n",
    "# ...\n",
    "# .. \n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Turn:\n",
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(28, 28, 1))\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "hidden1 = Dense(10, activation='relu')(pool2)\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "model.summary()\n",
    "plot_model(model, to_file='cnn.png', show_shapes=True)\n",
    "Image('cnn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: \n",
    "Build the following network: (LeNet5)\n",
    "- Conv2D(filters = 6, kernel_size = 5, activation = 'relu')\n",
    "- MaxPooling2D(pool_size = 2, strides = 2)\n",
    "- Conv2D(filters = 16, kernel_size = 5, activation = 'relu')\n",
    "- MaxPooling2D(pool_size = 2, strides = 2)\n",
    "- Flatten\n",
    "- Dense(84)\n",
    "- Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your model goes here..\n",
    "# ...\n",
    "# .. \n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the network \n",
    "We now define the network, we do not consider the output nodes yet.\n",
    "A single RNN cell is shown in the figure below in the middle:\n",
    "\n",
    "<img style=\"width: 70%;\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\"/>\n",
    "\n",
    "The joining of the two lines coming from the previous state $h_{t-1}$ and the current x-values $x_t$ is a concatenation to a vector  $[h_{t-1}, x_{t}]$ of size `state_size + num_classes_in`. Alternatively, instead of concatenating, one could also use two matrices $W_x$ and $W_h$ and keep the states separate. This is mathematically completely identical. The new state $h_t$ is \n",
    "then calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    h_{t} = \\tanh([h_{t-1}, x_{t}] \\cdot W + b) = \\tanh(h_{t-1} \\cdot W_h + x_{t} \\cdot U + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(100, 1))\n",
    "hidden1 = LSTM(10)(visible)  \n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "model.summary()\n",
    "plot_model(model, to_file='rnn.png', show_shapes=True)\n",
    "Image('rnn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "visible = Input(shape=(64, 64, 1))\n",
    "\n",
    "# first feature extractor\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "flat1 = Flatten()(pool1)\n",
    "\n",
    "# second feature extractor\n",
    "conv2 = Conv2D(16, kernel_size=8, activation='relu')(visible)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flat2 = Flatten()(pool2)\n",
    "\n",
    "# merge feature extractors\n",
    "merge = concatenate([flat1, flat2])\n",
    "\n",
    "# interpretation layer\n",
    "hidden1 = Dense(10, activation='relu')(merge)\n",
    "\n",
    "# prediction output\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='shared_input_layer.png', show_shapes=True)\n",
    "Image('shared_input_layer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Feature Extraction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "visible = Input(shape=(100, 1))\n",
    "\n",
    "# feature extraction\n",
    "extract1 = LSTM(10)(visible)\n",
    "\n",
    "# first interpretation model\n",
    "interp1 = Dense(10, activation='relu')(extract1)\n",
    "\n",
    "# second interpretation model\n",
    "interp21 = Dense(10, activation='relu')(extract1)\n",
    "interp22 = Dense(20, activation='relu')(interp21)\n",
    "interp23 = Dense(10, activation='relu')(interp22)\n",
    "\n",
    "# merge interpretation\n",
    "merge = concatenate([interp1, interp23])\n",
    "\n",
    "# output\n",
    "output = Dense(1, activation='sigmoid')(merge)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='shared_feature_extractor.png', show_shapes=True)\n",
    "\n",
    "Image('shared_feature_extractor.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# first input model\n",
    "visible1 = Input(shape=(64, 64, 1))\n",
    "conv11 = Conv2D(32, kernel_size=4, activation='relu')(visible1)\n",
    "pool11 = MaxPooling2D(pool_size=(2, 2))(conv11)\n",
    "conv12 = Conv2D(16, kernel_size=4, activation='relu')(pool11)\n",
    "pool12 = MaxPooling2D(pool_size=(2, 2))(conv12)\n",
    "flat1 = Flatten()(pool12)\n",
    "\n",
    "# second input model\n",
    "visible2 = Input(shape=(32, 32, 3))\n",
    "conv21 = Conv2D(32, kernel_size=4, activation='relu')(visible2)\n",
    "pool21 = MaxPooling2D(pool_size=(2, 2))(conv21)\n",
    "conv22 = Conv2D(16, kernel_size=4, activation='relu')(pool21)\n",
    "pool22 = MaxPooling2D(pool_size=(2, 2))(conv22)\n",
    "flat2 = Flatten()(pool22)\n",
    "\n",
    "# merge input models\n",
    "merge = concatenate([flat1, flat2])\n",
    "\n",
    "# interpretation model\n",
    "hidden1 = Dense(10, activation='relu')(merge)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='multiple_inputs.png', show_shapes=True)\n",
    "Image('multiple_inputs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multipe Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "visible = Input(shape=(100, 1))\n",
    "\n",
    "# feature extraction\n",
    "# return_sequences=True\n",
    "extract = LSTM(10, return_sequences=True)(visible)\n",
    "\n",
    "# classification output\n",
    "class11 = LSTM(10)(extract)\n",
    "class12 = Dense(10, activation='relu')(class11)\n",
    "output1 = Dense(1, activation='sigmoid')(class12)\n",
    "\n",
    "# sequence output\n",
    "output2 = TimeDistributed(Dense(1, activation='linear'))(extract)\n",
    "\n",
    "model = Model(inputs=visible, outputs=[output1, output2])\n",
    "model.summary()\n",
    "plot_model(model, to_file='multiple_output.png', show_shapes=True)\n",
    "Image('multiple_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced: Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "source": [
    "Mathematically, the GCN model follows this formula:\n",
    "\n",
    "$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$\n",
    "\n",
    "Here, $H^{(l)}$ denotes the $l^{th}$ layer in the network,\n",
    "$\\sigma$ is the non-linearity, and $W$ is the weight matrix for\n",
    "this layer. $D$ and $A$, as commonly seen, represent degree\n",
    "matrix and adjacency matrix, respectively. The ~ is a renormalization trick\n",
    "in which we add a self-connection to each node of the graph, and build the\n",
    "corresponding degree and adjacency matrix.  The shape of the input\n",
    "$H^{(0)}$ is $N \\times D$, where $N$ is the number of nodes\n",
    "and $D$ is the number of input features. We can chain up multiple\n",
    "layers as such to produce a node-level representation output with shape\n",
    "$N \\times F$, where $F$ is the dimension of the output node\n",
    "feature vector.\n",
    "\n",
    "The equation can be efficiently implemented using sparse matrix\n",
    "multiplication kernels (such as Kipf's\n",
    "`https://github.com/tkipf/pygcn`). The above DGL implementation\n",
    "in fact has already used this trick due to the use of builtin functions. To\n",
    "understand what is under the hood, please read the tutorial on page rank specified in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"keras-deep-graph-learning\" ] ; then git clone https://github.com/ypeleg/keras-deep-graph-learning; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "from tachles import fix_gcn_paths, load_mutag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "fix_gcn_paths()\n",
    "import keras_dgl\n",
    "from keras_dgl.layers import MultiGraphCNN, MultiGraphAttentionCNN\n",
    "from examples.utils import normalize_adj_numpy, evaluate_preds, preprocess_edge_adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "A, A_orig, X, Y, num_edge_features, num_graph_nodes, num_graphs, orig_num_graph_nodes, orig_num_graphs = load_mutag()\n",
    "print X.shape, Y.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "lines_to_next_cell": 0,
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "def plot_graph(adjacency_matrix):\n",
    "    rows, cols = np.where(adjacency_matrix == 1)\n",
    "    edges = zip(rows.tolist(), cols.tolist())\n",
    "    gr = nx.Graph()\n",
    "    gr.add_edges_from(edges)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    nx.draw_networkx(gr, ax=ax, with_labels=False, node_size=5, width=.5)\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "lines_to_next_cell": 0,
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "print X[0]\n",
    "# plot_graph(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "source": [
    "----\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/vermaMachineLearning/keras-deep-graph-learning/blob/master/keras_dgl/layers/multi_graph_cnn_layer.py#L9)</span>\n",
    "## MutliGraphCNN\n",
    "\n",
    "```python\n",
    "MutliGraphCNN(output_dim, num_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "MutliGraphCNN assumes that the number of nodes for each graph in the dataset is same. For graph with arbitrary size, one can simply append appropriate zero rows or columns in adjacency matrix (and node feature matrix) based on max graph size in the dataset to achieve this uniformity.\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "- __output_dim__: Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).\n",
    "- __num_filters__: Positive integer, number of graph filters used for constructing  __graph_conv_filters__ input.\n",
    "- __activation__: Activation function to use\n",
    ".\n",
    "If you don't specify anything, no activation is applied\n",
    "(ie. \"linear\" activation: `a(x) = x`).\n",
    "- __use_bias__: Boolean, whether the layer uses a bias vector.\n",
    "- __kernel_initializer__: Initializer for the `kernel` weights matrix\n",
    ".\n",
    "- __bias_initializer__: Initializer for the bias vector\n",
    ".\n",
    "- __kernel_regularizer__: Regularizer function applied to\n",
    "the `kernel` weights matrix\n",
    ".\n",
    "- __bias_regularizer__: Regularizer function applied to the bias vector\n",
    ".\n",
    "- __activity_regularizer__: Regularizer function applied to\n",
    "the output of the layer (its \"activation\").\n",
    ".\n",
    "- __kernel_constraint__: Constraint function applied to the kernel matrix\n",
    ".\n",
    "- __bias_constraint__: Constraint function applied to the bias vector\n",
    ".\n",
    "\n",
    "__Input shapes__\n",
    "\n",
    "* __graph node feature matrix__ input as a 3D tensor with shape: `(batch_size, num_graph_nodes, input_dim)` corresponding to graph node input feature matrix for each graph.<br />\n",
    "* __graph_conv_filters__ input as a 3D tensor with shape: `(batch_size, num_filters*num_graph_nodes, num_graph_nodes)` <br />\n",
    "`num_filters` is different number of graph convolution filters to be applied on graph. For instance `num_filters` could be power of graph Laplacian.<br />\n",
    "\n",
    "__Output shape__\n",
    "\n",
    "* 3D tensor with shape: `(batch_size, num_graph_nodes, output_dim)`\trepresenting convoluted output graph node embedding matrix for each graph in batch size.<br />\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/vermaMachineLearning/keras-deep-graph-learning/blob/master/examples/multi_gcnn_graph_classification_example.py)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "source": [
    "## The model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "num_filters = num_edge_features\n",
    "graph_conv_filters = preprocess_edge_adj_tensor(A, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "X_input = Input(shape=(X.shape[1], X.shape[2]))\n",
    "graph_conv_filters_input = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n",
    "\n",
    "# Your code here!\n",
    "\n",
    "output = Dense(Y.shape[1])(output)\n",
    "output = Activation('softmax')(output)\n",
    "\n",
    "nb_epochs = 200\n",
    "batch_size = 169\n",
    "\n",
    "model = Model(inputs=[X_input, graph_conv_filters_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "model.fit([X, graph_conv_filters], Y, batch_size=batch_size, validation_split=0.1, epochs=nb_epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "source": [
    "__References__: <br />\n",
    "[1] Kipf, Thomas N., and Max Welling. \"Semi-supervised classification with graph convolutional networks.\" arXiv preprint arXiv:1609.02907 (2016). <br />\n",
    "[2] Defferrard, Micha√´l, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" In Advances in Neural Information Processing Systems, pp. 3844-3852. 2016. <br />\n",
    "[3] Simonovsky, Martin, and Nikos Komodakis. \"Dynamic edge-conditioned filters in convolutional neural networks on graphs.\" In Proc. CVPR. 2017. <br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
